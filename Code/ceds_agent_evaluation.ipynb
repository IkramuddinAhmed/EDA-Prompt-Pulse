{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain | tail -n 1\n",
        "!pip install langchain-community | tail -n 1\n",
        "!pip install faiss-cpu | tail -n 1\n",
        "!pip install networkx | tail -n 1\n",
        "!pip install openai | tail -n 1\n",
        "!pip install matplotlib | tail -n 1\n",
        "!pip install langchain_mistralai | tail -n 1\n",
        "!pip install pdfplumber | tail -n 1\n",
        "!pip install sentence-transformers | tail -n 1\n",
        "!pip install transformers | tail -n 1\n",
        "!pip install langchain-huggingface | tail -n 1\n",
        "!pip install datasets | tail -n 1\n",
        "!pip install bert-score | tail -n 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SNclVLdC-Sx",
        "outputId": "8899c963-b566-4038-f075-3102691d4692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import datetime\n",
        "from typing import List, Dict\n",
        "\n",
        "# Data handling and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import pdfplumber\n",
        "\n",
        "# NLP and ML\n",
        "import spacy\n",
        "import torch\n",
        "\n",
        "# LangChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Google Colab and API integration\n",
        "from google.colab import userdata, auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "JpcfG4DwC5Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load NLP model for entity extraction\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_entities(text: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Extract entities from text and categorize them, including custom categories.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    entities = {\n",
        "        \"strategy\": [],\n",
        "        \"indicator\": [],\n",
        "        \"region\": [],\n",
        "        \"stakeholder\": [],\n",
        "        \"counties\": [],\n",
        "        \"Strengths\": [],\n",
        "        \"Weaknesses\": [],\n",
        "    }\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in [\"ORG\", \"GPE\", \"LOC\"]:\n",
        "            entities[\"region\"].append(ent.text)\n",
        "        elif ent.label_ == \"PERSON\":\n",
        "            entities[\"stakeholder\"].append(ent.text)\n",
        "        elif ent.label_ in [\"EVENT\", \"WORK_OF_ART\"]:\n",
        "            entities[\"strategy\"].append(ent.text)\n",
        "        else:\n",
        "            entities[\"indicator\"].append(ent.text)\n",
        "\n",
        "    counties = [ent.text for ent in doc.ents if \"County\" in ent.text or \"Parish\" in ent.text]\n",
        "    entities[\"counties\"].extend(counties)\n",
        "\n",
        "    strengths_keywords = [\"strength\", \"advantage\", \"opportunity\"]\n",
        "    weaknesses_keywords = [\"weakness\", \"challenge\", \"threat\", \"risk\"]\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        sentence_text = sentence.text.lower()\n",
        "        if any(keyword in sentence_text for keyword in strengths_keywords):\n",
        "            entities[\"Strengths\"].append(sentence.text.strip())\n",
        "        if any(keyword in sentence_text for keyword in weaknesses_keywords):\n",
        "            entities[\"Weaknesses\"].append(sentence.text.strip())\n",
        "\n",
        "    for key in entities:\n",
        "        entities[key] = list(set(entities[key]))\n",
        "    return entities\n",
        "\n",
        "def load_and_chunk_documents(file_paths: List[str]) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load and chunk multiple documents into manageable text chunks.\n",
        "    \"\"\"\n",
        "    all_documents = []\n",
        "    for file_path in file_paths:\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                text = \"\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
        "            if not text.strip():\n",
        "                raise ValueError(f\"The PDF {file_path} contains no readable text.\")\n",
        "\n",
        "            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "            chunks = splitter.create_documents([text], metadatas=[{\"source\": file_path}])\n",
        "            all_documents.extend(chunks)\n",
        "            logging.info(f\"Document '{os.path.basename(file_path)}' split into {len(chunks)} chunks.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process PDF {file_path}: {e}\")\n",
        "    return all_documents\n",
        "\n",
        "def vectorize_and_store(documents: List[Document]) -> FAISS:\n",
        "    \"\"\"\n",
        "    Vectorize documents and store in a FAISS vector store.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
        "        vector_store = FAISS.from_documents(documents, model)\n",
        "        vector_store.save_local(\"faiss_index\")\n",
        "        logging.info(\"Documents vectorized and saved in FAISS.\")\n",
        "        return vector_store\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to vectorize documents: {e}\")\n",
        "        raise\n",
        "\n",
        "def generate_knowledge_graph(documents: List[Document]) -> nx.DiGraph:\n",
        "    \"\"\"\n",
        "    Create a simple knowledge graph from documents.\n",
        "    \"\"\"\n",
        "    graph = nx.DiGraph()\n",
        "    for i, doc in enumerate(documents):\n",
        "        graph.add_node(f\"Doc {i+1}\", content=doc.page_content[:100] + \"...\")\n",
        "        if i > 0:\n",
        "            graph.add_edge(f\"Doc {i}\", f\"Doc {i+1}\")\n",
        "    logging.info(\"Knowledge graph created.\")\n",
        "    return graph\n",
        "\n",
        "def enhance_knowledge_graph(graph: nx.DiGraph, documents: List[Document]):\n",
        "    \"\"\"\n",
        "    Enhance the knowledge graph with entity-based relationships.\n",
        "    \"\"\"\n",
        "    for i, doc in enumerate(documents):\n",
        "        entities = extract_entities(doc.page_content)\n",
        "        doc_node = f\"Doc {i+1}\"\n",
        "        for category, entity_list in entities.items():\n",
        "            for entity in set(entity_list):\n",
        "                entity_node = f\"{category}: {entity}\"\n",
        "                if entity_node not in graph:\n",
        "                    graph.add_node(entity_node, type=category)\n",
        "                graph.add_edge(doc_node, entity_node)\n",
        "    logging.info(\"Knowledge graph enhanced with entity-based relationships.\")\n",
        "    return graph\n",
        "\n",
        "def visualize_enhanced_knowledge_graph(graph: nx.DiGraph):\n",
        "    \"\"\"\n",
        "    Visualize the enhanced knowledge graph with entity nodes.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    pos = nx.spring_layout(graph)\n",
        "    entity_nodes = [n for n, d in graph.nodes(data=True) if \"type\" in d]\n",
        "    doc_nodes = [n for n in graph if n not in entity_nodes]\n",
        "\n",
        "    nx.draw_networkx_nodes(graph, pos, nodelist=doc_nodes, node_color=\"lightblue\", node_size=3000)\n",
        "    nx.draw_networkx_nodes(graph, pos, nodelist=entity_nodes, node_color=\"lightgreen\", node_size=2000)\n",
        "    nx.draw_networkx_edges(graph, pos, arrows=True)\n",
        "    nx.draw_networkx_labels(graph, pos, font_size=8, font_weight=\"bold\")\n",
        "    plt.title(\"Enhanced Document Knowledge Graph\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"enhanced_knowledge_graph.png\", dpi=300)\n",
        "    logging.info(\"Enhanced knowledge graph visualization saved.\")\n",
        "\n",
        "def query_with_knowledge_graph(graph: nx.DiGraph, query: str, retriever, qa_chain):\n",
        "    \"\"\"\n",
        "    Use the knowledge graph to improve context retrieval for QA.\n",
        "    \"\"\"\n",
        "    query_entities = extract_entities(query)\n",
        "    relevant_nodes = set()\n",
        "    for category, entity_list in query_entities.items():\n",
        "        for entity in entity_list:\n",
        "            entity_node = f\"{category}: {entity}\"\n",
        "            if entity_node in graph:\n",
        "                relevant_nodes.update(nx.ancestors(graph, entity_node))\n",
        "\n",
        "    relevant_docs = []\n",
        "    for node in relevant_nodes:\n",
        "        if node.startswith(\"Doc\"):\n",
        "            relevant_docs.append(graph.nodes[node][\"content\"])\n",
        "\n",
        "    context = \" \".join(relevant_docs)\n",
        "    result = qa_chain.invoke({\"query\": query, \"context\": context})\n",
        "    return result[\"result\"]"
      ],
      "metadata": {
        "id": "_MG_140LC7Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Evaluation Framework Class\n",
        "class CEDSAgentTester:\n",
        "    def __init__(self, kg, chunks, llm, sheet_name: str = \"CEDS_Agent_Test_Results\"):\n",
        "        \"\"\"\n",
        "        Initialize the CEDSAgentTester with the knowledge graph, chunks, and LLM for synthesis.\n",
        "        \"\"\"\n",
        "        self.kg = kg\n",
        "        self.chunks = chunks\n",
        "        self.llm = llm\n",
        "        self.sheet_name = sheet_name\n",
        "        self.setup_google_sheets()\n",
        "\n",
        "    def setup_google_sheets(self):\n",
        "        \"\"\"\n",
        "        Set up connection to Google Sheets with proper authentication.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Authenticate Colab\n",
        "            auth.authenticate_user()\n",
        "\n",
        "            # Get Google Sheets credentials\n",
        "            creds, _ = default()\n",
        "            self.gc = gspread.authorize(creds)\n",
        "\n",
        "            # Create or open spreadsheet\n",
        "            try:\n",
        "                self.sheet = self.gc.open(self.sheet_name)\n",
        "            except:\n",
        "                self.sheet = self.gc.create(self.sheet_name)\n",
        "\n",
        "            # Setup main worksheet\n",
        "            try:\n",
        "                self.worksheet = self.sheet.worksheet(\"Test Results\")\n",
        "            except:\n",
        "                self.worksheet = self.sheet.add_worksheet(\"Test Results\", 1000, 8)\n",
        "                headers = [\n",
        "                    \"Date\", \"Test ID\", \"Category\", \"Agent Prompt\", \"User Query\",\n",
        "                    \"Output\", \"Evaluation Score\", \"Notes\"\n",
        "                ]\n",
        "                self.worksheet.insert_row(headers, 1)\n",
        "\n",
        "            logging.info(f\"Successfully connected to sheet: {self.sheet_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error setting up Google Sheets: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "    def evaluate_response(self, response: str, category: str) -> tuple:\n",
        "        \"\"\"\n",
        "        Evaluate the quality of the agent's response.\n",
        "        Returns score (1-5) and notes.\n",
        "        \"\"\"\n",
        "        metrics = {\n",
        "            \"length\": len(response.split()) >= 50,\n",
        "            \"relevance\": any(word in response.lower() for word in category.lower().split()),\n",
        "            \"structure\": response.count('.') >= 2,\n",
        "            \"specificity\": any(char.isdigit() for char in response)\n",
        "        }\n",
        "\n",
        "        score = sum(metrics.values()) + 1  # Base score of 1 plus metrics (max score 5)\n",
        "        notes = []\n",
        "\n",
        "        if not metrics[\"length\"]:\n",
        "            notes.append(\"Response too brief\")\n",
        "        if not metrics[\"relevance\"]:\n",
        "            notes.append(\"Low relevance to category\")\n",
        "        if not metrics[\"structure\"]:\n",
        "            notes.append(\"Poor response structure\")\n",
        "        if not metrics[\"specificity\"]:\n",
        "            notes.append(\"Lacks specific details\")\n",
        "\n",
        "        return score, \"; \".join(notes) if notes else \"Good response\"\n",
        "\n",
        "    def log_evaluation(self, query, response, score, notes):\n",
        "        \"\"\"\n",
        "        Log evaluation results to Google Sheets.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = {\n",
        "                \"Date\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"Test ID\": f\"Test_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\",\n",
        "                \"Category\": \"General\",\n",
        "                \"Agent Prompt\": \"LLM Response\",\n",
        "                \"User Query\": query,\n",
        "                \"Output\": response,\n",
        "                \"Evaluation Score\": score,\n",
        "                \"Notes\": notes\n",
        "            }\n",
        "\n",
        "            # Append results to Google Sheets\n",
        "            self.worksheet.append_row([\n",
        "                result[\"Date\"], result[\"Test ID\"], result[\"Category\"],\n",
        "                result[\"Agent Prompt\"], result[\"User Query\"], result[\"Output\"],\n",
        "                result[\"Evaluation Score\"], result[\"Notes\"]\n",
        "            ])\n",
        "\n",
        "            logging.info(f\"Logged evaluation for query: {query}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to log evaluation: {str(e)}\")"
      ],
      "metadata": {
        "id": "Emc3GTBoMkSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the pipeline.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_paths = ['/content/indiana15_ceds_2023.pdf', '/content/tbrcp_ceds_2022.pdf']\n",
        "        file_paths = [path.strip() for path in file_paths if path.strip()]\n",
        "\n",
        "        logging.info(\"Loading and chunking documents...\")\n",
        "        documents = load_and_chunk_documents(file_paths)\n",
        "\n",
        "        logging.info(\"Vectorizing documents...\")\n",
        "        vector_store = vectorize_and_store(documents)\n",
        "\n",
        "        logging.info(\"Generating and visualizing knowledge graph...\")\n",
        "        knowledge_graph = generate_knowledge_graph(documents)\n",
        "        knowledge_graph = enhance_knowledge_graph(knowledge_graph, documents)\n",
        "        # visualize_enhanced_knowledge_graph(knowledge_graph)\n",
        "\n",
        "        logging.info(\"Initializing QA...\")\n",
        "        retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "        llm = ChatMistralAI(api_key=userdata.get('mistral_api'), model_name=\"mistral-large-latest\")\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            retriever=retriever,\n",
        "            chain_type=\"stuff\",\n",
        "            return_source_documents=False\n",
        "        )\n",
        "\n",
        "        # Initialize the CEDSAgentTester for evaluation\n",
        "        tester = CEDSAgentTester(knowledge_graph, documents, llm)\n",
        "\n",
        "        while True:\n",
        "            query = input(\"Ask a question (type 'exit' to quit): \")\n",
        "            if query.lower() == \"exit\":\n",
        "                logging.info(\"Exiting...\")\n",
        "                break\n",
        "            response = query_with_knowledge_graph(knowledge_graph, query, retriever, qa_chain)\n",
        "            print(\"\\nResponse:\", response, \"\\n\")\n",
        "\n",
        "            # Evaluate and log response\n",
        "            score, notes = tester.evaluate_response(response, \"General\")\n",
        "            tester.log_evaluation(query, response, score, notes)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in pipeline: {e}\")"
      ],
      "metadata": {
        "id": "OR5T8ZTfMkfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "rPineIqcnEDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d3d229-e73c-4b14-c802-2f39c6ff13dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (type 'exit' to quit): Compare the highlights of tampa vs indiana 15\n",
            "\n",
            "Response: Based on the provided context, here are some highlights comparing the Tampa Bay region and the Indiana 15 region:\n",
            "\n",
            "**Tampa Bay Region:**\n",
            "- **Economic Data (2020):**\n",
            "  - Average wages: $58,413\n",
            "  - Employment growth rate: 3.9%\n",
            "  - Population growth rate: 6.3%\n",
            "- **Industry Concentration (2022 Location Quotients):**\n",
            "  - Apparel manufacturing: 1.93\n",
            "  - Insurance carriers and related activities: 1.83\n",
            "  - Telecommunications: 1.72\n",
            "- **Economic Development:**\n",
            "  - Focus on attracting tourists and enhancing arts.\n",
            "\n",
            "**Indiana 15 Region:**\n",
            "- **Economic Challenges:**\n",
            "  - Lack of childcare options.\n",
            "  - Limited access to broadband.\n",
            "  - Changes in business operations.\n",
            "  - Lack of support services for mental health.\n",
            "- **Regional Planning:**\n",
            "  - Launched a regional resiliency and recovery planning process in 2021.\n",
            "  - Kicked off the CEDS process in November 2022 with a phase focused on information gathering.\n",
            "\n",
            "**Differences:**\n",
            "- The provided context does not include statistical economic data for Indiana 15, so a direct comparison with Tampa Bay's economic data is not possible.\n",
            "- Indiana 15 is focused on addressing specific economic challenges and regional planning efforts, while Tampa Bay's highlights center around economic data and industry concentration. \n",
            "\n",
            "Ask a question (type 'exit' to quit): How is the demographic composition of Tampa changing, including age distribution, racial/ethnic diversity, and migration patterns?\n",
            "\n",
            "Response: Based on the provided context, the demographic composition of Tampa Bay is changing in the following ways:\n",
            "\n",
            "1. **Age Distribution**: Tampa Bay is experiencing a 'twin-peaked' distribution of age cohorts. This means there are two significant concentration of population based on age:\n",
            "   - One peak is centered around the 65- to 69-year-olds, suggesting a significant in-migration of older individuals.\n",
            "   - The second peak is centered around 30- to 34-year-olds, with an overall increase in the 25- to 49-year-old population. This suggests an in-migration of prime working-age individuals.\n",
            "\n",
            "2. **Racial/Ethnic Diversity**: The context mentions that Tampa Bay's population is growing and becoming more racially diverse. However, it does not provide specific details about the changes in racial/ethnic diversity.\n",
            "\n",
            "3. **Migration Patterns**:\n",
            "   - There is a significant in-migration of older individuals, centering around the 65- to 69-year-old age group.\n",
            "   - There is also a notable in-migration of working-age individuals, particularly those aged 25 to 49.\n",
            "   - The population growth is primarily driven by domestic migration. \n",
            "\n",
            "Ask a question (type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2xSUDjIZO_LJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}